<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter [
<!-- Some useful entities borrowed from HTML -->
<!ENTITY ndash  "&#x2013;">
<!ENTITY mdash  "&#x2014;">
<!ENTITY hellip "&#x2026;">
<!ENTITY plusmn "&#xB1;">
]>
<chapter xmlns="http://docbook.org/ns/docbook"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink"
    version="5.0"
    xml:id="lay_of_the_land">
    <?dbhtml stop-chunking?>
    <title>Lay of the Land</title>
    <para>This section helps you set up your working environment and use it to
        take a look around your cloud.</para>
    <section xml:id="dashboard_admin">
        <title>Using the OpenStack Dashboard for Administration</title>
        <para>As a cloud administrative user, you can use the OpenStack
            Dashboard to create and manage projects, users, images, and flavors
            as an admin user. Users are allowed to create anad manage images within
            specified projects and share images, depending on the Image Service
            configuration. Typically the policy configuration only allows admin
            users to set quotas and create and manage services. The Dashboard
            provides an <guilabel>Admin</guilabel> tab with a <guilabel>System
            Panel</guilabel> and <guilabel>Identity Panel</guilabel>. These
            interfaces give you access to system information and usage as well
            as configuring what end-users can do. Refer to the <link
                xlink:href="http://docs.openstack.org/user-guide-admin/content/ch_dashboard.html"
                >OpenStack Admin User Guide</link> for detailed
        how-to information about using the Dashboard as an admin user.</para>
    </section>
    <section xml:id="cli_tools">
        <title>Command Line Tools</title>
        <para>We recommend using a combination of the OpenStack
            command line interface (CLI) tools and the
            OpenStack Dashboard for administration. Some users with a background in other
            cloud technologies may be using the EC2 Compatibility API,
            which uses somewhat different naming conventions from the
            native API. We highlight those differences.</para>
        <para>We strongly suggest that you install the command-line
            clients from the <link
                xlink:href="https://pypi.python.org/"
                >Python Package
                Index</link> (PyPI) (https://pypi.python.org/) instead
            of from the distribution packages. The clients are
            under heavy development and it is very likely at any given
            time the version of the packages distributed by your
            operating system vendor are out of date.</para>
        <para>The "pip" utility is used to manage package installation
            from the PyPI archive and is available in the "python-pip"
            package in most Linux distributions. Each OpenStack
            project has its own client, so depending on which services
            your site runs, install some or all of the following
            packages:</para>
        <itemizedlist role="compact">
            <listitem>
                <para>python-novaclient (<glossterm>nova</glossterm>
                    CLI)</para>
            </listitem>
            <listitem>
                <para>python-glanceclient
                        (<glossterm>glance</glossterm> CLI)</para>
            </listitem>
            <listitem>
                <para>python-keystoneclient
                        (<glossterm>keystone</glossterm> CLI)</para>
            </listitem>
            <listitem>
                <para>python-cinderclient
                        (<glossterm>cinder</glossterm> CLI)</para>
            </listitem>
            <listitem>
                <para>python-swiftclient (<glossterm>swift</glossterm>
                    CLI)</para>
            </listitem>
            <listitem>
                <para>python-neutronclient
                        (<glossterm>neutron</glossterm> CLI)</para>
            </listitem>
        </itemizedlist>
        <section xml:id="install_tools">
            <title>Installing the Tools</title>
            <para>To install (or upgrade) a package from the PyPI
                archive with pip, as root:</para>
            <programlisting><?db-font-size 60%?># pip install [--upgrade] &lt;package-name&gt;</programlisting>
            <para>To remove the package:</para>
            <programlisting><?db-font-size 60%?># pip uninstall &lt;package-name&gt;</programlisting>
            <para>If you need even newer versions of the clients, pip
                can install directly from the upstream git repository
                using the <code>-e</code> flag. You must specify a
                name for the Python egg that is installed. For
                example:</para>
            <programlisting><?db-font-size 60%?># pip install -e git+https://github.com/openstack/python-novaclient.git#egg=python-novaclient</programlisting>
            <para>If you support the EC2 API on your cloud you should
                also install the "euca2ools" package or some other EC2
                API tool so you can get the same view your users have.
                Using EC2 API based tools is mostly out of the scope
                of this guide, though we discuss getting credentials
                for use with it.</para>
        </section>
        <section xml:id="admin_cli">
            <title>Administrative Command Line Tools</title>
            <para>There are also several <emphasis>*-</emphasis>manage
                command line tools. These are installed with the project's
                services on the cloud controller and do not need to be installed
                separately:</para>
            <itemizedlist role="compact">
                <listitem>
                    <para>nova-manage</para>
                </listitem>
                <listitem>
                    <para>glance-manage</para>
                </listitem>
                <listitem>
                    <para>keystone-manage</para>
                </listitem>
                <listitem>
                    <para>cinder-manage</para>
                </listitem>
            </itemizedlist>
            <para>Unlike the CLI tools mentioned above, the
                    <code>*-manage</code> tools must be run from the
                cloud controller, as root, because they need read
                access to the config files such as
                    <code>/etc/nova/nova.conf</code> and make queries
                directly against the database rather than against the
                OpenStack <glossterm>API endpoint</glossterm>s.</para>
            <warning>
                <para>The existence of the <code>*-manage</code> tools is
                a legacy issue. It is a goal of the OpenStack project
                to eventually migrate all of the remaining
                functionality in the <code>*-manage</code> tools into
                the API-based tools. Until that day, you need to
                SSH into the <glossterm>cloud controller
                    node</glossterm> to perform some maintenance
                operations that require one of the
                    <code>*-manage</code> tools.</para>
            </warning>
        </section>
        <?hard-pagebreak?>
        <section xml:id="get_creds">
            <title>Getting Credentials</title>
            <para>You must have the appropriate credentials if you wish to use
                the command line tools to make queries against your OpenStack
                cloud. By far the easiest way to obtain
                    <glossterm>authentication</glossterm> credentials to use
                with command line clients is to use the OpenStack Dashboard.
                From the top right navigation row, select
                    <guimenuitem>Project</guimenuitem>, then <guimenuitem>Access
                    &amp; Security</guimenuitem>, then <guimenuitem>API
                    Access</guimenuitem> to access the user settings page where
                you can set your language and timezone preferences for the
                dashboard view. This action displays two buttons,
                    <guilabel>Download OpenStack RC File</guilabel> and
                    <guilabel>Download EC2 Credentials</guilabel>, which let you
                to generate files you can source in your shell to populate the
                environment variables the command line tools need to know where
                your service endpoints are as well as your authentication
                information. The user you logged into the dashboard dictates
                the filename for the openrc file, such as <filename>demo-openrc.sh</filename>. When logged
                in as admin, the file is named <filename>admin-openrc.sh</filename>.</para>
            <para>The generated file looks something like
                this:</para>
            <programlisting><?db-font-size 60%?>#!/bin/bash

# With the addition of Keystone, to use an openstack cloud you should
# authenticate against keystone, which returns a **Token** and **Service
# Catalog**. The catalog contains the endpoint for all services the
# user/tenant has access to - including nova, glance, keystone, swift.
#
# *NOTE*: Using the 2.0 *auth api* does not mean that compute api is 2.0.
# We use the 1.1 *compute api*
export OS_AUTH_URL=http://203.0.113.10:5000/v2.0

# With the addition of Keystone we have standardized on the term **tenant**
# as the entity that owns the resources.
export OS_TENANT_ID=98333aba48e756fa8f629c83a818ad57
export OS_TENANT_NAME="test-project"

# In addition to the owning entity (tenant), openstack stores the entity
# performing the action as the **user**.
export OS_USERNAME=demo

# With Keystone you pass the keystone password.
echo "Please enter your OpenStack Password: "
read -s OS_PASSWORD_INPUT
export OS_PASSWORD=$OS_PASSWORD_INPUT</programlisting>
            <warning>
                <para>This does not save your password in plain text, which is a
                    good thing. But when you source or run the script, it
                    prompts for your password and then stores your response in
                    the environment variable <code>OS_PASSWORD</code>. It is
                    important to note that this does require interactivity. It
                    is possible to store a value directly in the script if you
                    require a noninteractive operation, but you then need to be
                    extremely cautious with the security and permissions of this
                    file.</para>
            </warning>
            <para>EC2 compatibility credentials can be downloaded from the
                    <guimenuitem>Project</guimenuitem>, then <guimenuitem>Access
                    &amp; Security</guimenuitem>, then <guimenuitem>API
                    Access</guimenuitem> to display the <guilabel>Download EC2
                    Credentials</guilabel> button. Click the button to generate
                a zip file with server x509 certificates and a shell script
                fragment. Create a new directory in a secure location because
                these are live credentials containing all the authentication
                information required to access your cloud identity, unlike the
                default <code>user-openrc</code>. Extract the zip file here. You
                should have <filename>cacert.pem</filename>,
                    <filename>cert.pem</filename>, <filename>ec2rc.sh</filename>
                and <filename>pk.pem</filename>. The
                    <filename>ec2rc.sh</filename> is similar to this:</para>
            <programlisting><?db-font-size 50%?>#!/bin/bash

NOVARC=$(readlink -f "${BASH_SOURCE:-${0}}" 2&gt;/dev/null) ||\
NOVARC=$(python -c 'import os,sys; \
print os.path.abspath(os.path.realpath(sys.argv[1]))' "${BASH_SOURCE:-${0}}")
NOVA_KEY_DIR=${NOVARC%/*}
export EC2_ACCESS_KEY=df7f93ec47e84ef8a347bbb3d598449a
export EC2_SECRET_KEY=ead2fff9f8a344e489956deacd47e818
export EC2_URL=http://203.0.113.10:8773/services/Cloud
export EC2_USER_ID=42 # nova does not use user id, but bundling requires it
export EC2_PRIVATE_KEY=${NOVA_KEY_DIR}/pk.pem
export EC2_CERT=${NOVA_KEY_DIR}/cert.pem
export NOVA_CERT=${NOVA_KEY_DIR}/cacert.pem
export EUCALYPTUS_CERT=${NOVA_CERT} # euca-bundle-image seems to require this

alias ec2-bundle-image="ec2-bundle-image --cert $EC2_CERT --privatekey \
$EC2_PRIVATE_KEY --user 42 --ec2cert $NOVA_CERT"
alias ec2-upload-bundle="ec2-upload-bundle -a $EC2_ACCESS_KEY -s \
$EC2_SECRET_KEY --url $S3_URL --ec2cert $NOVA_CERT"</programlisting>
            <para>To put the EC2 credentials into your environment source the
                    <code>ec2rc.sh</code> file.</para>
        </section>
        <section xml:id="cli_tricks">
            <title>Inspecting API Calls</title>
            <para>The command line tools can be made to show the
                OpenStack API calls they make by passing the
                <code>--debug</code> flag to them. For example:</para>
            <programlisting><?db-font-size 60%?><prompt>#</prompt> nova --debug list</programlisting>
            <para>This example shows the HTTP requests from the client
                and the responses from the endpoints, which can be
                helpful in creating custom tools written to the
                OpenStack API.</para>
            <tip><para><link
                    xlink:href="https://wiki.openstack.org/wiki/KeyringSupport"
                    >Keyring Support</link>
                (https://wiki.openstack.org/wiki/KeyringSupport)
                enables you to securely save your OpenStack password
                in an encrypted file.
            </para>
            <para>This feature is disabled by default. To enable it,
                add the <code>--os-cache</code> flag or set the
                environment variable <code>OS_CACHE=1</code>.</para>
                <para>Configuring OS_CACHE causes the command line tool to
                    authenticate on each and every interaction with
                    the cloud. This can assist with working around this
                    scenario. However, it increases the time taken to run commands
                    and also the load on the server.</para>
            </tip>
            <section xml:id="curl">
                <title>Using cURL for Further Inspection</title>
                <para>Underlying the use of the command line tools is
                    the OpenStack API, which is a RESTful API that
                    runs over HTTP. There may be cases where you want
                    to interact with the API directly or need to use
                    it because of a suspected bug in one of the CLI
                    tools. The best way to do this is use a
                    combination of <link
                        xlink:href="http://curl.haxx.se/"
                        >cURL</link>
                    (http://curl.haxx.se/) and another tool to parse
                    the JSON, such as <link
                        xlink:href="http://stedolan.github.com/jq/"
                    >jq</link> (http://stedolan.github.com/jq/),
                    from the responses.</para>
                <para>The first thing you must do is authenticate with
                    the cloud using your credentials to get an
                        <glossterm>authentication token</glossterm>.</para>
                <para>Your credentials are a combination of username,
                    password, and tenant (project). You can extract
                    these values from the <code>openrc.sh</code>
                    discussed above. The token allows you to interact
                    with your other service endpoints without needing
                    to re-authenticate for every request. Tokens are
                    typically good for 24 hours, and when the token
                    expires, you are alerted with a 401 (Unauthorized)
                    response and you can request another token.</para>
                <para>
                    <orderedlist>
                        <listitem>
                            <para>Look at your OpenStack service
                                   <glossterm>catalog</glossterm>:</para>
                            <programlisting language="bash"><?db-font-size 55%?>
<prompt>$</prompt> curl -s -X POST http://203.0.113.10:35357/v2.0/tokens \
-d '{"auth": {"passwordCredentials": {"username":"test-user", "password":"test-password"}, "tenantName":"test-project"}}' \
-H "Content-type: application/json" | jq .</programlisting>
                        </listitem>
                        <listitem>
                            <para>Read through the JSON response to
                                get a feel for how the catalog is laid
                                out.</para>
                            <para>To make working with subsequent
                                requests easier, store the token in an
                                environment variable.</para>
                            <programlisting language="bash"><?db-font-size 55%?>
<prompt>$</prompt> TOKEN=`curl -s -X POST http://203.0.113.10:35357/v2.0/tokens \
-d '{"auth": {"passwordCredentials": {"username":"test-user", "password":"test-password"}, "tenantName":"test-project"}}' \
-H "Content-type: application/json" |  jq -r .access.token.id`</programlisting>
                            <para>Now you can refer to your token on
                                the command line as $TOKEN.</para>
                        </listitem>
                        <listitem>
                            <para>Pick a service endpoint from your
                                service catalog, such as compute, and
                                try out a request like listing
                                instances (servers).</para>
                            <programlisting><?db-font-size 60%?>
<prompt>$</prompt> curl -s \
-H "X-Auth-Token: $TOKEN" \
http://203.0.113.10:8774/v2/98333aba48e756fa8f629c83a818ad57/servers | jq .</programlisting>

                        </listitem>
                    </orderedlist>
                </para>
                <para>To discover how API requests should be
                    structured, read the <link
                        xlink:href="http://api.openstack.org/api-ref.html"
                        >OpenStack API Reference</link>
                    (http://api.openstack.org/api-ref.html). To chew
                    through the responses using jq, see the <link
                        xlink:href="http://stedolan.github.com/jq/manual/"
                        >jq Manual</link>
                    (http://stedolan.github.com/jq/manual/).</para>
                <para>The <code>-s flag</code> used in the cURL
                    commands above are used to prevent the progress
                    meter from being shown. If you are having trouble
                    running cURL commands, you'll want to remove it.
                    Likewise, to help you troubleshoot cURL commands
                    you can include the <code>-v</code> flag to show
                    you the verbose output. There are many more
                    extremely useful features in cURL, refer to the
                    man page for all of the options.</para>
            </section>
        </section>
        <section xml:id="servers_services">
            <title>Servers and Services</title>
            <para>As an administrator, there are a few ways to
                discover what your OpenStack cloud looks like simply
                by using the OpenStack tools available. This section
                gives you an idea of how to get an overview of your
                cloud, its shape, size, and current state.</para>
            <para>First, you can discover what servers belong to your
                OpenStack cloud by running:</para>
            <programlisting><?db-font-size 60%?><prompt>#</prompt> nova-manage service list | sort</programlisting>
            <para>The output looks like the following:</para>
            <programlisting><?db-font-size 60%?>Binary           Host              Zone Status  State Updated_At
nova-cert        cloud.example.com nova enabled  :-)  2013-02-25 19:32:38
nova-compute     c01.example.com   nova enabled  :-)  2013-02-25 19:32:35
nova-compute     c02.example.com   nova enabled  :-)  2013-02-25 19:32:32
nova-compute     c03.example.com   nova enabled  :-)  2013-02-25 19:32:36
nova-compute     c04.example.com   nova enabled  :-)  2013-02-25 19:32:32
nova-compute     c05.example.com   nova enabled  :-)  2013-02-25 19:32:41
nova-conductor   cloud.example.com nova enabled  :-)  2013-02-25 19:32:40
nova-consoleauth cloud.example.com nova enabled  :-)  2013-02-25 19:32:36
nova-network     cloud.example.com nova enabled  :-)  2013-02-25 19:32:32
nova-scheduler   cloud.example.com nova enabled  :-)  2013-02-25 19:32:33</programlisting>
            <para>The output shows that there are five compute nodes
                and one cloud controller. You see a smiley face like
                    <code>:-)</code> which indicates that the services
                are up and running and functional. If a service is no
                longer available, the <code>:-)</code> changes to an
                    <code>XXX</code>. This is an indication that you
                should troubleshoot why the service is down.</para>
            <para>If you are using Cinder, run the following command
                to see a similar listing:</para>
            <programlisting><?db-font-size 60%?><prompt>#</prompt> cinder-manage host list | sort</programlisting>
            <programlisting><?db-font-size 60%?>host              zone
c01.example.com   nova
c02.example.com   nova
c03.example.com   nova
c04.example.com   nova
c05.example.com   nova
cloud.example.com nova</programlisting>
            <para>With these two tables, you now have a good overview
                of what servers and services make up your
                cloud.</para>
            <para>You can also use the Identity Service (Keystone), to
                see what services are available in your cloud as well
                as what endpoints have been configured for the
                services.</para>
            <para>The following command requires you to have your
                shell environment configured with the proper
                administrative variables.</para>

            <programlisting><?db-font-size 60%?><prompt>$</prompt> keystone catalog</programlisting>

            <programlisting><?db-font-size 60%?>Service: image
+-------------+----------------------------------------+
|   Property  |                 Value                  |
+-------------+----------------------------------------+
|   adminURL  | http://cloud.internal.example.com:9292 |
| internalURL | http://cloud.example.com:9292          |
|  publicURL  | http://cloud.example.com:9292          |
|    region   |               RegionOne                |
+-------------+----------------------------------------+

Service: identity
+-------------+----------------------------------------------+
|   Property  |                   Value                      |
+-------------+----------------------------------------------+
|   adminURL  | http://cloud.internal.example.com:35357/v2.0 |
| internalURL | http://cloud.example.com:5000/v2.0           |
|  publicURL  | http://cloud.example.com:5000/v2.0           |
|    region   |                 RegionOne                    |
+-------------+----------------------------------------------+</programlisting>
            <para>The output above has been truncated to show only two
                services. You will see one service block for each
                service that your cloud provides. Note how the
                endpoint domain can be different depending on the
                endpoint type. Different endpoint domains per type are
                not required, but can be done for different reasons
                such as endpoint privacy or network traffic
                segregation.</para>
            <para>You can find the version of the Compute installation by using the
                    <command>nova-manage</command> command:
                <screen><prompt>#</prompt> <userinput>nova-manage version list</userinput></screen></para>
        </section>
        <section xml:id="diagnose-compute">
            <title>Diagnose your compute nodes</title>
            <para>You can obtain extra information about the running
                virtual machines: their CPU usage, the memory, the disk I/O or
                network I/O, per instance, by running the <command>nova
                    diagnostics</command> command with a server ID:</para>
            <screen><prompt>$</prompt> <userinput>nova diagnostics &lt;serverID&gt;</userinput></screen>
            <para>The output of this command will vary depending on the
                hypervisor, as they support different attributes. The following
                demonstrates the difference between the two most popular hypervisors.
                 Example output when the hypervisor is Xen:
                <screen><computeroutput>
+----------------+-----------------+
|    Property    |      Value      |
+----------------+-----------------+
| cpu0           | 4.3627          |
| memory         | 1171088064.0000 |
| memory_target  | 1171088064.0000 |
| vbd_xvda_read  | 0.0             |
| vbd_xvda_write | 0.0             |
| vif_0_rx       | 3223.6870       |
| vif_0_tx       | 0.0             |
| vif_1_rx       | 104.4955        |
| vif_1_tx       | 0.0             |
+----------------+-----------------+</computeroutput></screen>
                While the command should work with any hypervisor that is
                controlled through libvirt (e.g., KVM, QEMU, LXC), it has only
                been tested with KVM. Example output when the hypervisor is
                KVM:</para>
            <screen><computeroutput>
+------------------+------------+
| Property         | Value      |
+------------------+------------+
| cpu0_time        | 2870000000 |
| memory           | 524288     |
| vda_errors       | -1         |
| vda_read         | 262144     |
| vda_read_req     | 112        |
| vda_write        | 5606400    |
| vda_write_req    | 376        |
| vnet0_rx         | 63343      |
| vnet0_rx_drop    | 0          |
| vnet0_rx_errors  | 0          |
| vnet0_rx_packets | 431        |
| vnet0_tx         | 4905       |
| vnet0_tx_drop    | 0          |
| vnet0_tx_errors  | 0          |
| vnet0_tx_packets | 45         |
+------------------+------------+</computeroutput></screen>
        </section>
    </section>
    <section xml:id="network">
        <title>Network Inspection</title>
        <para>To see what Fixed IP networks are configured in your cloud, you can use
            the <command>nova</command> command-line client to get the IP
            ranges.<screen><prompt>$</prompt> <userinput>nova network-list</userinput>
<computeroutput>+--------------------------------------+--------+--------------+
| ID                                   | Label  | Cidr         |
+--------------------------------------+--------+--------------+
| 3df67919-9600-4ea8-952e-2a7be6f70774 | test01 |  10.1.0.0/24 |
| 8283efb2-e53d-46e1-a6bd-bb2bdef9cb9a | test02 |  10.1.1.0/24 |
+--------------------------------------+--------+--------------+</computeroutput></screen></para>
        <para>The <command>nova-manage</command> tool can provide some additional details.</para>

        <screen><prompt>#</prompt> <userinput>nova-manage network list</userinput>
<computeroutput>id IPv4        IPv6 start address DNS1 DNS2 VlanID project   uuid 
1  10.1.0.0/24 None 10.1.0.3      None None 300    2725bbd   beacb3f2
2  10.1.1.0/24 None 10.1.1.3      None None 301    none      d0b1a796</computeroutput></screen>
        <para>This output shows that two networks are configured, each
            network containing 255 IPs (a /24 subnet). The first
            network has been assigned to a certain project while the
            second network is still open for assignment. You can
            assign this network manually or it is automatically
            assigned when a project launches their first
            instance.</para>
        <para>To find out if any floating IPs are available in your
            cloud, run:</para>

        <programlisting><?db-font-size 60%?><prompt>#</prompt> nova-manage floating list</programlisting>

        <programlisting><?db-font-size 55%?>2725bbd458e2459a8c1bd36be859f43f 1.2.3.4 None                                 nova vlan20
None                             1.2.3.5 48a415e7-6f07-4d33-ad00-814e60b010ff nova vlan20</programlisting>
        <para>Here, two floating IPs are available. The first has been
            allocated to a project while the other is
            unallocated.</para>
    </section>
    <section xml:id="users_projects">
        <title>Users and Projects</title>
        <para>To see a list of projects that have been added to the
            cloud, run:</para>

        <programlisting><?db-font-size 60%?><prompt>$</prompt> keystone tenant-list</programlisting>

        <programlisting><?db-font-size 60%?>+-----+----------+---------+
| id  | name     | enabled |
+-----+----------+---------+
| ... | jtopjian | True    |
| ... | alvaro   | True    |
| ... | everett  | True    |
| ... | admin    | True    |
| ... | services | True    |
| ... | jonathan | True    |
| ... | lorin    | True    |
| ... | anne     | True    |
| ... | rhulsker | True    |
| ... | tom      | True    |
| ... | adam     | True    |
+-----+----------+---------+</programlisting>
        <?hard-pagebreak?>
        <para>To see a list of users, run:</para>

        <programlisting><?db-font-size 60%?><prompt>$</prompt> keystone user-list</programlisting>

        <programlisting><?db-font-size 60%?>+-----+----------+---------+------------------------------+
| id  | name     | enabled | email                        |
+-----+----------+---------+------------------------------+
| ... | everett  | True    | everett.towne@backspace.com  |
| ... | jonathan | True    | jon@sfcu.edu                 |
| ... | nova     | True    | nova@localhost               |
| ... | rhulsker | True    | ryan.hulkster@cyberalbert.ca |
| ... | lorin    | True    | lorinhoch@nsservices.com     |
| ... | alvaro   | True    | Alvaro.Perry@cyberalbert.ca  |
| ... | anne     | True    | anne.green@backspace.com     |
| ... | admin    | True    | root@localhost               |
| ... | cinder   | True    | cinder@localhost             |
| ... | glance   | True    | glance@localhost             |
| ... | jtopjian | True    | joe.topjian@cyberalbert.com  |
| ... | adam     | True    | adam@ossmanuals.net          |
| ... | tom      | True    | fafield@univm.edu.au         |
+-----+----------+---------+------------------------------+</programlisting>
        <note>
            <para>Sometimes a user and a group have a one-to-one
                mapping. This happens for standard system accounts,
                such as cinder, glance, nova, and swift, or when only
                one user is ever part of a group.</para>
        </note>
    </section>
    <section xml:id="running_instances">
        <title>Running Instances</title>
        <para>To see a list of running instances, run:</para>

        <programlisting><?db-font-size 60%?><prompt>$</prompt> nova list --all-tenants</programlisting>

        <programlisting><?db-font-size 60%?>+-----+------------------+--------+-------------------------------------------+
| ID  | Name             | Status | Networks                                  |
+-----+------------------+--------+-------------------------------------------+
| ... | Windows          | ACTIVE | novanetwork_1=10.1.1.3, 199.116.232.39    |
| ... | cloud controller | ACTIVE | novanetwork_0=10.1.0.6; jtopjian=10.1.2.3 |
| ... | compute node 1   | ACTIVE | novanetwork_0=10.1.0.4; jtopjian=10.1.2.4 |
| ... | devbox           | ACTIVE | novanetwork_0=10.1.0.3                    |
| ... | devstack         | ACTIVE | novanetwork_0=10.1.0.5                    |
| ... | initial          | ACTIVE | nova_network=10.1.7.4, 10.1.8.4           |
| ... | lorin-head       | ACTIVE | nova_network=10.1.7.3, 10.1.8.3           |
+-----+------------------+--------+-------------------------------------------+</programlisting>
        <para>Unfortunately this command does not tell you various
            details about the running instances, such as what compute
            node the instance is running on, what flavor the instance
            is, and so on. You can use the following command to view
            details about individual instances:</para>

        <programlisting><?db-font-size 60%?><prompt>$</prompt> nova show &lt;uuid&gt;</programlisting>
        <?hard-pagebreak?>
        <para>For example:
            <programlisting><?db-font-size 60%?># nova show 81db556b-8aa5-427d-a95c-2a9a6972f630</programlisting><programlisting><?db-font-size 60%?>+-------------------------------------+-----------------------------------+
| Property                            | Value                             |
+-------------------------------------+-----------------------------------+
| OS-DCF:diskConfig                   | MANUAL                            |
| OS-EXT-SRV-ATTR:host                | c02.example.com                   |
| OS-EXT-SRV-ATTR:hypervisor_hostname | c02.example.com                   |
| OS-EXT-SRV-ATTR:instance_name       | instance-00000029                 |
| OS-EXT-STS:power_state              | 1                                 |
| OS-EXT-STS:task_state               | None                              |
| OS-EXT-STS:vm_state                 | active                            |
| accessIPv4                          |                                   |
| accessIPv6                          |                                   |
| config_drive                        |                                   |
| created                             | 2013-02-13T20:08:36Z              |
| flavor                              | m1.small (6)                      |
| hostId                              | ...                               |
| id                                  | ...                               |
| image                               | Ubuntu 12.04 cloudimg amd64 (...) |
| key_name                            | jtopjian-sandbox                  |
| metadata                            | {}                                |
| name                                | devstack                          |
| novanetwork_0 network               | 10.1.0.5                          |
| progress                            | 0                                 |
| security_groups                     | [{u'name': u'default'}]           |
| status                              | ACTIVE                            |
| tenant_id                           | ...                               |
| updated                             | 2013-02-13T20:08:59Z              |
| user_id                             | ...                               |
+-------------------------------------+-----------------------------------+</programlisting></para>
        <para>The above output shows that an instance named
            <userinput>devstack</userinput> was created from an Ubuntu 12.04 image using a flavor
            of m1.small and is hosted on the compute node
            c02.example.com.</para>
    </section>
    <section xml:id="ops-lay-of-land-summary">
        <title>Summary</title>
        <para>We hope you have enjoyed this quick tour of your working
    environment, including how to interact with your cloud and extract
    useful information. From here, you can use the <link
                xlink:href=" http://docs.openstack.org/user-guide-admin/content/"
                    ><citetitle>Admin User Guide</citetitle></link>
    as your reference for all of the command line functionality in your
    cloud.</para>
    </section>
</chapter>
