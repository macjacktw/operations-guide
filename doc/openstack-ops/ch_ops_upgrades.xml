<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:id="ch_ops_upgrades"
         xmlns="http://docbook.org/ns/docbook"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:ns5="http://www.w3.org/2000/svg"
         xmlns:ns4="http://www.w3.org/1998/Math/MathML"
         xmlns:ns3="http://www.w3.org/1999/xhtml"
         xmlns:ns="http://docbook.org/ns/docbook">
  <title>Upgrades</title>

  <para>With the exception of Object Storage, upgrading from one version of
  OpenStack to another can take a great deal of effort. Until the situation
  improves, this chapter provides some guidance on the operational aspects
  that you should consider for performing an upgrade based on detailed steps
  for a basic architecture.</para>

  <section xml:id="ops_upgrades-pre-testing">
    <title>Pre-Upgrade Testing Environment</title>

    <para>Probably the most important step of all is the pre-upgrade testing.
    Especially if you are upgrading immediately after release of a new
    version, undiscovered bugs might hinder your progress. Some deployers
    prefer to wait until the first point release is announced. However, if you
    have a significant deployment, you might follow the development and
    testing of the release, thereby ensuring that bugs for your use cases are
    fixed.<indexterm class="singular">
        <primary>upgrading</primary>

        <secondary>pre-upgrade testing</secondary>
      </indexterm></para>

    <para>Each OpenStack cloud is different, and as a result, even with what
    may seem a near-identical architecture to this guide, you must still test
    upgrades between versions in your environment. For this, you need an
    approximate clone of your environment.</para>

    <para>However, that is not to say that it needs to be the same size or use
    identical hardware as the production environment—few of us have that
    luxury. It is important to consider the hardware and scale of the cloud
    you are upgrading, but here are some tips to avoid that incredible
    cost:<indexterm class="singular">
        <primary>upgrading</primary>

        <secondary>controlling cost of</secondary>
      </indexterm></para>

    <variablelist>
      <varlistentry>
        <term>Use your own cloud</term>

        <listitem>
          <para>The simplest place to start testing the next version of
          OpenStack is by setting up a new environment inside your own cloud.
          This may seem odd—especially the double virtualization used in
          running compute nodes—but it's a sure way to very quickly test your
          configuration.</para>
        </listitem>
      </varlistentry>

      <varlistentry>
        <term>Use a public cloud</term>

        <listitem>
          <para>Especially because your own cloud is unlikely to have
          sufficient space to scale test to the level of the entire cloud,
          consider using a public cloud to test the scalability limits of your
          cloud controller configuration. Most public clouds bill by the hour,
          which means it can be inexpensive to perform even a test with many
          nodes.<indexterm class="singular">
              <primary>cloud controllers</primary>

              <secondary>scalability and</secondary>
            </indexterm></para>
        </listitem>
      </varlistentry>

      <varlistentry>
        <term>Make another storage endpoint on the same system</term>

        <listitem>
          <para>If you use an external storage plug-in or shared file system
          with your cloud, in many cases, it's possible to test that it works
          by creating a second share or endpoint. This will enable you to test
          the system before entrusting the new version onto your
          storage.</para>
        </listitem>
      </varlistentry>

      <varlistentry>
        <term>Watch the network</term>

        <listitem>
          <para>Even at smaller-scale testing, it should be possible to
          determine whether something is going horribly wrong in
          intercomponent communication if you look at the network packets and
          see too many.</para>
        </listitem>
      </varlistentry>
    </variablelist>

    <para>To actually set up the test environment, there are several methods.
    Some prefer to do a full manual install using the <!-- the following link suffers from a toolchain problem, where in the
        rendered PDF version, the title butts up against the link, which comes
        before the title FIXME --> <emphasis><link
    xlink:href="http://opsgui.de/NPFTC8">OpenStack Installation
    Guides</link></emphasis> and then see what the final configuration files
    look like and which packages were installed. Others prefer to create a
    clone of their automated configuration infrastructure with changed package
    repository URLs and then alter the configuration until it starts working.
    Either approach is valid, and which you use depends on experience.</para>

    <para>An upgrade pre-testing system is excellent for getting the
    configuration to work; however, it is important to note that the
    historical use of the system and differences in user interaction can
    affect the success of upgrades, too. We've seen experiences where database
    migrations encountered a bug (later fixed!) because of slight table
    differences between fresh Grizzly installs and those that migrated from
    Folsom to Grizzly.</para>

    <para>Artificial scale testing can go only so far. Once your cloud is
    upgraded, you'll also need to pay careful attention to the performance
    aspects of your cloud.</para>
  </section>

  <section xml:id="ops_upgrades-prepare-roll-back">
    <title>Preparing for a Rollback</title>

    <para>Like all major system upgrades, your upgrade could fail for one or
    more difficult-to-determine reasons. You should prepare for this situation
    by leaving the ability to roll back your environment to the previous
    release, including databases, configuration files, and packages. We
    provide an example process for rolling back your environment in <xref
    linkend="ops_upgrades-roll-back" />.<indexterm class="singular">
        <primary>upgrading</primary>

        <secondary>process overview</secondary>
      </indexterm><indexterm class="singular">
        <primary>rollbacks</primary>

        <secondary>preparing for</secondary>
      </indexterm><indexterm class="singular">
        <primary>upgrading</primary>

        <secondary>preparation for</secondary>
      </indexterm></para>
  </section>

  <?hard-pagebreak ?>

  <section xml:id="ops_upgrades-general-steps">
    <title>Upgrades</title>

    <para>The upgrade process generally follows these steps:</para>

    <orderedlist>
      <listitem>
        <para>Perform some "cleaning" of the environment prior to starting the
        upgrade process to ensure a consistent state. For example, instances
        not fully purged from the system after deletion may cause
        indeterminate behavior.</para>
      </listitem>

      <listitem>
        <para>Read the release notes and documentation.</para>
      </listitem>

      <listitem>
        <para>Find incompatibilities between your versions.</para>
      </listitem>

      <listitem>
        <para>Develop an upgrade procedure and assess it thoroughly using a
        test environment similar to your production environment.</para>
      </listitem>

      <listitem>
        <para>Run the upgrade procedure on the production environment.</para>
      </listitem>
    </orderedlist>

    <para>You can perform an upgrade with operational instances, but this
    strategy can be dangerous. You might consider using live migration to
    temporarily relocate instances to other compute nodes while performing
    upgrades. However, you must ensure database consistency throughout the
    process; otherwise your environment may become unstable. Also, don't
    forget to provide sufficient notice to your users, including giving them
    plenty of time to perform their own backups.</para>

    <para>The following order for service upgrades seems the most
    successful:</para>

    <orderedlist>
      <listitem>
        <para>Upgrade the OpenStack Identity Service (keystone).</para>
      </listitem>

      <listitem>
        <para>Upgrade the OpenStack Image Service (glance).</para>
      </listitem>

      <listitem>
        <para>Upgrade OpenStack Compute (nova), including networking
        components.</para>
      </listitem>

      <listitem>
        <para>Upgrade OpenStack Block Storage (cinder).</para>
      </listitem>

      <listitem>
        <para>Upgrade the OpenStack dashboard.</para>
      </listitem>
    </orderedlist>

    <para>The general upgrade process includes the following steps:</para>

    <orderedlist>
      <listitem>
        <para>Create a backup of configuration files and databases.</para>
      </listitem>

      <listitem>
        <para>Update the configuration files according to the release
        notes.</para>
      </listitem>

      <listitem>
        <para>Upgrade the packages using your distribution's package
        manager.</para>
      </listitem>

      <listitem>
        <para>Stop services, update database schemas, and restart
        services.</para>
      </listitem>

      <listitem>
        <para>Verify proper operation of your environment.</para>
      </listitem>
    </orderedlist>
  </section>

  <section xml:id="ops_upgrades_grizzly_havana-ubuntu">
    <title>How to Perform an Upgrade from Grizzly to Havana—Ubuntu</title>

    <?dbhtml stop-chunking?>

    <para>For this section, we assume that you are starting with the
    architecture provided in the OpenStack <link
    xlink:href="http://opsgui.de/NPGunp">Installation Guide</link> and
    upgrading to the same architecture for Havana. All nodes should run Ubuntu
    12.04 LTS. This section primarily addresses upgrading core OpenStack
    services, such as the Identity Service (keystone), Image Service (glance),
    Compute (nova) including networking, Block Storage (cinder), and the
    dashboard.<indexterm class="startofrange" xml:id="UPubuntu">
        <primary>upgrading</primary>

        <secondary>Grizzly to Havana (Ubuntu)</secondary>
      </indexterm></para>

    <section xml:id="upgrade_impact_users-ubuntu">
      <title>Impact on Users</title>

      <para>The upgrade process will interrupt management of your environment,
      including the dashboard. If you properly prepare for this upgrade,
      tenant instances will continue to operate normally.</para>
    </section>

    <section xml:id="upgrade_considerations-ubuntu">
      <title>Upgrade Considerations</title>

      <para>Always review the <link
      xlink:href="http://opsgui.de/1eLzHFY">release notes</link> before
      performing an upgrade to learn about newly available features that you
      may want to enable and deprecated features that you should
      disable.</para>
    </section>

    <section xml:id="upgrade_backup-ubuntu">
      <title>Perform a Backup</title>

      <para>Save the configuration files on all nodes, as shown here:</para>

      <screen><prompt>#</prompt> <userinput>for i in keystone glance nova cinder openstack-dashboard</userinput>
<prompt>&gt;</prompt> <userinput>do mkdir $i-grizzly</userinput>
<prompt>&gt;</prompt> <userinput>done</userinput>
<prompt>#</prompt> <userinput>for i in keystone glance nova cinder openstack-dashboard</userinput>
<prompt>&gt;</prompt> <userinput>do cp -r /etc/$i/* $i-grizzly/</userinput>
<prompt>&gt;</prompt> <userinput>done</userinput></screen>

      <note>
        <para>You can modify this example script on each node to handle
        different services.</para>
      </note>

      <para>Back up all databases on the controller:</para>

      <screen><prompt>#</prompt> <userinput>mysqldump -u root -p --opt --add-drop-database \
--all-databases &gt; grizzly-db-backup.sql</userinput></screen>
    </section>

    <section xml:id="upgrade_manage_repos-ubuntu">
      <title>Manage Repositories</title>

      <para>On all nodes, remove the repository for Grizzly packages and add
      the repository for Havana packages:</para>

      <screen><prompt>#</prompt> <userinput>apt-add-repository -r cloud-archive:grizzly</userinput>
<prompt>#</prompt> <userinput>apt-add-repository cloud-archive:havana</userinput></screen>

      <warning>
        <para>Make sure any automatic updates are disabled.</para>
      </warning>
    </section>

    <section xml:id="upgrade_update_configuration-ubuntu">
      <title>Update Configuration Files</title>

      <para>Update the glance configuration on the controller node for
      compatibility with <phrase role="keep-together">Havana</phrase>.</para>

      <para>If not currently present and configured as follows, add or modify
      the following keys in <filename>/etc/glance/glance-api.conf</filename>
      and <filename>/etc/glance/glance-registry.conf</filename>:</para>

      <programlisting language="ini">[keystone_authtoken]
auth_uri = http://controller:5000
auth_host = controller
admin_tenant_name = service
admin_user = glance
admin_password = GLANCE_PASS

[paste_deploy]
flavor = keystone</programlisting>

      <para>If currently present, remove the following key from the
      <literal>[filter:authtoken]</literal> section in
      <filename>/etc/glance/glance-api-paste.ini</filename> and
      <filename>/etc/glance/glance-registry-paste.ini</filename>:</para>

      <programlisting language="ini">[filter:authtoken]
flavor = keystone</programlisting>

      <para>Update the nova configuration on all nodes for compatibility with
      Havana.</para>

      <para>Add the new <literal>[database]</literal>section and associated
      key to <filename>/etc/nova/nova.conf</filename>:</para>

      <programlisting language="ini">[database]
connection = mysql://nova:NOVA_DBPASS@controller/nova</programlisting>

      <para>Remove defunct configuration from the <literal>[DEFAULT]</literal>
      section in <filename>/etc/nova/nova.conf</filename>:</para>

      <programlisting language="ini">[DEFAULT]
sql_connection = mysql://nova:NOVA_DBPASS@controller/nova</programlisting>

      <para>If not already present and configured as follows, add or modify
      the following keys in <filename>/etc/nova/nova.conf</filename>:</para>

      <programlisting language="ini">[keystone_authtoken]
auth_uri = http://controller:5000/v2.0
auth_host = controller
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = nova
admin_password = NOVA_PASS</programlisting>

      <para>On all compute nodes, increase the DHCP lease time (measured in
      seconds) in <filename>/etc/nova/nova.conf</filename> to enable currently
      active instances to continue leasing their IP addresses during the
      upgrade process:</para>

      <programlisting language="ini">[DEFAULT]
dhcp_lease_time = 86400
</programlisting>

      <warning>
        <para>Setting this value too high may cause more dynamic environments
        to run out of available IP addresses. Use an appropriate value for
        your environment.</para>
      </warning>

      <para>You must restart dnsmasq and the networking component of Compute
      to enable the new DHCP lease time:</para>

      <screen><prompt>#</prompt> <userinput>pkill -9 dnsmasq</userinput>
<prompt>#</prompt> <userinput>service nova-network restart</userinput></screen>

      <para>Update the Cinder configuration on the controller and storage
      nodes for compatibility with Havana.</para>

      <para>Add the new <literal>[database]</literal> section and associated
      key to <filename>/etc/cinder/cinder.conf</filename>:</para>

      <programlisting language="ini">[database]
connection = mysql://cinder:CINDER_DBPASS@controller/cinder</programlisting>

      <para>Remove defunct configuration from the <literal>[DEFAULT]</literal>
      section in <filename>/etc/cinder/cinder.conf</filename>:</para>

      <programlisting language="ini">[DEFAULT]
sql_connection = mysql://cinder:CINDER_DBPASS@controller/cinder</programlisting>

      <para>If not currently present and configured as follows, add or modify
      the following key in
      <filename>/etc/cinder/cinder.conf</filename>:</para>

      <programlisting language="ini">[keystone_authtoken]
auth_uri = http://controller:5000</programlisting>

      <para>Update the dashboard configuration on the controller node for
      compatibility with Havana.</para>

      <para>The dashboard installation procedure and configuration file
      changed substantially between Grizzly and Havana. Particularly, if you
      are running Django 1.5 or later, you must ensure that
      <filename>/etc/openstack-dashboard/local_settings</filename> contains a
      correctly configured <literal>ALLOWED_HOSTS</literal> key that contains
      a list of hostnames recognized by the dashboard.</para>

      <para>If users will access your dashboard using
      <emphasis>http://dashboard.example.com</emphasis>, you would set:</para>

      <programlisting language="ini">ALLOWED_HOSTS=['dashboard.example.com']</programlisting>

      <para>If users will access your dashboard on the local system, you would
      set:</para>

      <programlisting language="ini">ALLOWED_HOSTS=['localhost']</programlisting>

      <para>If users will access your dashboard using an IP address in
      addition to a hostname, you would set:</para>

      <programlisting language="ini">ALLOWED_HOSTS=['dashboard.example.com', '192.168.122.200']</programlisting>
    </section>

    <section xml:id="upgrade_packages_controller-ubuntu">
      <title>Upgrade Packages on the Controller Node</title>

      <para>Upgrade packages on the controller node to Havana, as shown
      below:</para>

      <screen><prompt>#</prompt> <userinput>apt-get update</userinput>
<prompt>#</prompt> <userinput>apt-get dist-upgrade</userinput></screen>

      <note>
        <para>Depending on your specific configuration, performing a
        <code>dist-upgrade</code> may restart services supplemental to your
        OpenStack environment. For example, if you use Open-iSCSI for Block
        Storage volumes and the upgrade includes a new <code>open-scsi</code>
        package, the package manager will restart Open-iSCSI services, which
        may cause the volumes for your users to be disconnected.</para>
      </note>

      <para>The package manager will ask you about updating various
      configuration files. We recommend denying these changes. The package
      manager will append <code>.dpkg-dist</code> to the end of newer versions
      of existing configuration files. You should consider adopting
      conventions associated with the newer configuration files and merging
      them with your existing configuration files after completing the upgrade
      process.</para>
    </section>

    <section xml:id="upgrade_database_restart-ubuntu">
      <title>Stop Services, Update Database Schemas, and Restart Services on
      the Controller Node</title>

      <para>Stop each service, run the database synchronization command if
      necessary to update the associated database schema, and restart each
      service to apply the new configuration. Some services require additional
      commands:</para>

      <variablelist>
        <varlistentry>
          <term>OpenStack Identity</term>

          <listitem>
            <screen><prompt>#</prompt> <userinput>service keystone stop</userinput>
<prompt>#</prompt> <userinput>keystone-manage token_flush</userinput>
<prompt>#</prompt> <userinput>keystone-manage db_sync</userinput>
<prompt>#</prompt> <userinput>service keystone start</userinput></screen>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>OpenStack Image Service</term>

          <listitem>
            <screen><prompt>#</prompt> <userinput>service glance-api stop</userinput>
<prompt>#</prompt> <userinput>service glance-registry stop</userinput>
<prompt>#</prompt> <userinput>glance-manage db_sync</userinput>
<prompt>#</prompt> <userinput>service glance-api start</userinput>
<prompt>#</prompt> <userinput>service glance-registry start</userinput></screen>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>OpenStack Compute</term>

          <listitem>
            <screen><prompt>#</prompt> <userinput>service nova-api stop</userinput>
<prompt>#</prompt> <userinput>service nova-scheduler stop</userinput>
<prompt>#</prompt> <userinput>service nova-conductor stop</userinput>
<prompt>#</prompt> <userinput>service nova-cert stop</userinput>
<prompt>#</prompt> <userinput>service nova-consoleauth stop</userinput>
<prompt>#</prompt> <userinput>service nova-novncproxy stop</userinput>
<prompt>#</prompt> <userinput>nova-manage db sync</userinput>
<prompt>#</prompt> <userinput>service nova-api start</userinput>
<prompt>#</prompt> <userinput>service nova-scheduler start</userinput>
<prompt>#</prompt> <userinput>service nova-conductor start</userinput>
<prompt>#</prompt> <userinput>service nova-cert start</userinput>
<prompt>#</prompt> <userinput>service nova-consoleauth start</userinput>
<prompt>#</prompt> <userinput>service nova-novncproxy start</userinput></screen>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>OpenStack Block Storage</term>

          <listitem>
            <screen><prompt>#</prompt> <userinput>service cinder-api stop</userinput>
<prompt>#</prompt> <userinput>service cinder-scheduler stop</userinput>
<prompt>#</prompt> <userinput>cinder-manage db sync</userinput>
<prompt>#</prompt> <userinput>service cinder-api start</userinput>
<prompt>#</prompt> <userinput>service cinder-scheduler start</userinput></screen>
          </listitem>
        </varlistentry>
      </variablelist>

      <para>The controller node update is complete. Now you can upgrade the
      compute nodes.</para>
    </section>

    <section xml:id="upgrade_packages_compute-ubuntu">
      <title>Upgrade Packages and Restart Services on the Compute
      Nodes</title>

      <para>Upgrade packages on the compute nodes to Havana:</para>

      <screen><prompt>#</prompt> <userinput>apt-get update</userinput>
<prompt>#</prompt> <userinput>apt-get dist-upgrade</userinput></screen>

      <note>
        <para>Make sure you have removed the repository for Grizzly packages
        and added the repository for Havana packages.</para>
      </note>

      <warning>
        <para>Due to a packaging issue, this command may fail with the
        following error:</para>

        <screen>Errors were encountered while processing:
    /var/cache/apt/archives/
      qemu-utils_1.5.0+dfsg-3ubuntu5~cloud0_amd64.deb
    /var/cache/apt/archives/
      qemu-system-common_1.5.0+dfsg-3ubuntu5~cloud0_
        amd64.deb
    E: Sub-process /usr/bin/dpkg
    returned an error code (1)</screen>

        <para>You can fix this issue by using the following command:</para>

        <screen><prompt>#</prompt> <userinput>apt-get -f install</userinput></screen>
      </warning>

      <para>The packaging system will ask about updating the
      <filename>/etc/nova/api-paste.ini</filename> file. As with the
      controller upgrade, we recommend denying these changes and reviewing the
      <code>.dpkg-dist</code> file after completing the upgrade
      process.</para>

      <para>To restart compute services:</para>

      <screen><prompt>#</prompt> <userinput>service nova-compute restart</userinput>
<prompt>#</prompt> <userinput>service nova-network restart</userinput>
<prompt>#</prompt> <userinput>service nova-api-metadata restart</userinput></screen>
    </section>

    <section xml:id="upgrade_packages_storage-ubuntu">
      <title>Upgrade Packages and Restart Services on the Block Storage
      Nodes</title>

      <para>Upgrade packages on the storage nodes to Havana:</para>

      <screen><prompt>#</prompt> <userinput>apt-get update</userinput>
<prompt>#</prompt> <userinput>apt-get dist-upgrade</userinput></screen>

      <note>
        <para>Make sure you have removed the repository for Grizzly packages
        and added the repository for Havana packages.</para>
      </note>

      <para>The packaging system will ask about updating the
      <filename>/etc/cinder/api-paste.ini</filename> file. Like the controller
      upgrade, we recommend denying these changes and reviewing the
      <code>.dpkg-dist</code> file after completing the upgrade
      process.</para>

      <?hard-pagebreak ?>

      <para>To restart Block Storage services:<indexterm class="endofrange"
      startref="UPubuntu" /></para>

      <screen><prompt>#</prompt> <userinput>service cinder-volume restart</userinput></screen>
    </section>
  </section>

  <section xml:id="ops_upgrades_grizzly_havana-rhel">
    <title>How to Perform an Upgrade from Grizzly to Havana—Red Hat Enterprise
    Linux and Derivatives</title>

    <?dbhtml stop-chunking?>

    <para>For this section, we assume that you are starting with the
    architecture provided in the OpenStack <link
    xlink:href="http://opsgui.de/NPGvrs">Installation Guide</link> and
    upgrading to the same architecture for Havana. All nodes should run Red
    Hat Enterprise Linux 6.4 or compatible derivatives. Newer minor releases
    should also work. This section primarily addresses upgrading core
    OpenStack services, such as the Identity Service (keystone), Image Service
    (glance), Compute (nova) including networking, Block Storage (cinder),
    and the dashboard.<indexterm class="startofrange" xml:id="UPredhat">
        <primary>upgrading</primary>

        <secondary>Grizzly to Havana (Red Hat)</secondary>
      </indexterm></para>

    <section xml:id="upgrade_impact_users-rhel">
      <title>Impact on Users</title>

      <para>The upgrade process will interrupt management of your environment,
      including the dashboard. If you properly prepare for this upgrade,
      tenant instances will continue to operate normally.</para>
    </section>

    <section xml:id="upgrade_considerations-rhel">
      <title>Upgrade Considerations</title>

      <para>Always review the <link
      xlink:href="http://opsgui.de/1eLzHFY">release notes</link> before
      performing an upgrade to learn about newly available features that you
      may want to enable and deprecated features that you should
      disable.</para>
    </section>

    <section xml:id="upgrade_backup-rhel">
      <title>Perform a Backup</title>

      <para>First, save the configuration files on all nodes:</para>

      <screen><prompt>#</prompt> <userinput>for i in keystone glance nova cinder openstack-dashboard</userinput>
<prompt>&gt;</prompt> <userinput>do mkdir $i-grizzly</userinput>
<prompt>&gt;</prompt> <userinput>done</userinput>
<prompt>#</prompt> <userinput>for i in keystone glance nova cinder openstack-dashboard</userinput>
<prompt>&gt;</prompt> <userinput>do cp -r /etc/$i/* $i-grizzly/</userinput>
<prompt>&gt;</prompt> <userinput>done</userinput></screen>

      <note>
        <para>You can modify this example script on each node to handle
        different services.</para>
      </note>

      <para>Next, back up all databases on the controller:</para>

      <screen><prompt>#</prompt> <userinput>mysqldump -u root -p --opt --add-drop-database \
  --all-databases &gt; grizzly-db-backup.sql</userinput></screen>
    </section>

    <section xml:id="upgrade_manage_repos-rhel">
      <title>Manage Repositories</title>

      <para>On all nodes, remove the repository for Grizzly packages and add
      the repository for Havana packages:</para>

      <screen><prompt>#</prompt> <userinput>yum erase rdo-release-grizzly</userinput>
<prompt>#</prompt> <userinput>yum install http://repos.fedorapeople.org/repos/openstack/openstack-havana/  \
rdo-release-havana-7.noarch.rpm</userinput></screen>

      <warning>
        <para>Make sure any automatic updates are disabled.</para>
      </warning>

      <note>
        <para>Consider checking for newer versions of the <link
        xlink:href="http://opsgui.de/1eLBXNB">Havana repository</link>.</para>
      </note>
    </section>

    <section xml:id="upgrade_update_configuration-rhel">
      <title>Update Configuration Files</title>

      <para>Update the glance configuration on the controller node for
      compatibility with <phrase role="keep-together">Havana</phrase>.</para>

      <para>If not currently present and configured as follows, add or modify
      the following keys in <filename>/etc/glance/glance-api.conf</filename>
      and <filename>/etc/glance/glance-registry.conf</filename>:</para>

      <screen><prompt>#</prompt> <userinput>openstack-config --set /etc/glance/glance-api.conf keystone_authtoken \
  auth_uri http://controller:5000</userinput>
<prompt>#</prompt> <userinput>openstack-config --set /etc/glance/glance-api.conf keystone_authtoken \
  auth_host controller</userinput>
<prompt>#</prompt> <userinput>openstack-config --set /etc/glance/glance-api.conf keystone_authtoken \
  admin_tenant_name service</userinput>
<prompt>#</prompt> <userinput>openstack-config --set /etc/glance/glance-api.conf keystone_authtoken \
  admin_user glance</userinput>
<prompt>#</prompt> <userinput>openstack-config --set /etc/glance/glance-api.conf keystone_authtoken \
  admin_password GLANCE_PASS</userinput>
<prompt>#</prompt> <userinput>openstack-config --set /etc/glance/glance-api.conf paste_deploy \
  flavor keystone</userinput></screen>

      <screen><prompt>#</prompt> <userinput>openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken \
  auth_uri http://controller:5000</userinput>
<prompt>#</prompt> <userinput>openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken \
  auth_host controller</userinput>
<prompt>#</prompt> <userinput>openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken \
  admin_tenant_name service</userinput>
<prompt>#</prompt> <userinput>openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken \
  admin_user glance</userinput>
<prompt>#</prompt> <userinput>openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken \
  admin_password GLANCE_PASS</userinput>
<prompt>#</prompt> <userinput>openstack-config --set /etc/glance/glance-registry.conf paste_deploy \
  flavor keystone</userinput></screen>

      <para>If currently present, remove the following key from the
      [filter:authtoken] section in
      <filename>/etc/glance/glance-api-paste.ini</filename> and
      <filename>/etc/glance/glance-registry-paste.ini</filename>:</para>

      <programlisting language="ini">[filter:authtoken]
flavor = keystone</programlisting>

      <para>Update the nova configuration on all nodes for compatibility with
      Havana.</para>

      <para>Add the new <literal>[database]</literal> section and associated
      key to <filename>/etc/nova/nova.conf</filename>:</para>

      <screen><prompt>#</prompt> <userinput>openstack-config --set /etc/nova/nova.conf database \
  connection mysql://nova:NOVA_DBPASS@controller/nova</userinput></screen>

      <para>Remove defunct database configuration from
      <filename>/etc/nova/nova.conf</filename>:</para>

      <screen><prompt>#</prompt> <userinput>openstack-config --del /etc/nova/nova.conf DEFAULT sql_connection</userinput></screen>

      <para>If not already present and configured as follows, add or modify
      the following keys in <filename>/etc/nova/nova.conf</filename>:</para>

      <screen><prompt>#</prompt> <userinput>openstack-config --set /etc/nova/nova.conf keystone_authtoken \
  auth_uri http://controller:5000/v2.0</userinput>
<prompt>#</prompt> <userinput>openstack-config --set /etc/nova/nova.conf keystone_authtoken \
  auth_host controller</userinput>
<prompt>#</prompt> <userinput>openstack-config --set /etc/nova/nova.conf keystone_authtoken \
  admin_tenant_name service</userinput>
<prompt>#</prompt> <userinput>openstack-config --set /etc/nova/nova.conf keystone_authtoken \
  admin_user nova</userinput>
<prompt>#</prompt> <userinput>openstack-config --set /etc/nova/nova.conf keystone_authtoken \
  admin_password NOVA_PASS</userinput></screen>

      <para>On all compute nodes, increase the DHCP lease time (measured in
      seconds) in <filename>/etc/nova/nova.conf</filename> to enable currently
      active instances to continue leasing their IP addresses during the
      upgrade process, as shown here:</para>

      <screen><prompt>#</prompt> <userinput>openstack-config --set /etc/nova/nova.conf DEFAULT \
  dhcp_lease_time 86400</userinput></screen>

      <warning>
        <para>Setting this value too high may cause more dynamic environments
        to run out of available IP addresses. Use an appropriate value for
        your environment.</para>
      </warning>

      <?hard-pagebreak ?>

      <para>You must restart dnsmasq and the nova networking service to enable
      the new DHCP lease time:</para>

      <screen><prompt>#</prompt> <userinput>pkill -9 dnsmasq</userinput>
<prompt>#</prompt> <userinput>service openstack-nova-network restart</userinput></screen>

      <para>Update the cinder configuration on the controller and storage
      nodes for compatibility with Havana.</para>

      <para>Add the new [database] section and associated key to
      <filename>/etc/cinder/cinder.conf</filename>:</para>

      <screen><prompt>#</prompt> <userinput>openstack-config --set /etc/cinder/cinder.conf database \
  connection mysql://cinder:CINDER_DBPASS@controller/cinder</userinput></screen>

      <para>Remove defunct database configuration from
      <filename>/etc/cinder/cinder.conf</filename>:</para>

      <screen><prompt>#</prompt> <userinput>openstack-config --del /etc/cinder/cinder.conf DEFAULT sql_connection</userinput></screen>

      <para>If not currently present and configured as follows, add or modify
      the following key in
      <filename>/etc/cinder/cinder.conf</filename>:</para>

      <screen><prompt>#</prompt> <userinput>openstack-config --set /etc/cinder/cinder.conf keystone_authtoken \
  auth_uri http://controller:5000</userinput></screen>

      <para>Update the dashboard configuration on the controller node for
      compatibility with Havana.</para>

      <para>The dashboard installation procedure and configuration file
      changed substantially between Grizzly and Havana. Particularly, if you
      are running Django 1.5 or later, you must ensure that
      <filename>/etc/openstack-dashboard/local_settings</filename> contains a
      correctly configured <literal>ALLOWED_HOSTS</literal> key that contains
      a list of hostnames recognized by the dashboard.</para>

      <para>If users will access your dashboard using
      <emphasis>http://dashboard.example.com</emphasis>, you would set:</para>

      <programlisting language="ini">ALLOWED_HOSTS=['dashboard.example.com']</programlisting>

      <para>If users will access your dashboard on the local system, you would
      set:</para>

      <programlisting language="ini">ALLOWED_HOSTS=['localhost']</programlisting>

      <para>If users will access your dashboard using an IP address in
      addition to a hostname, you would set:</para>

      <programlisting language="ini">ALLOWED_HOSTS=['dashboard.example.com', '192.168.122.200']</programlisting>
    </section>

    <section xml:id="upgrade_packages_controller-rhel">
      <title>Upgrade Packages on the Controller Node</title>

      <para>Upgrade packages on the controller node to Havana:</para>

      <screen><prompt>#</prompt> <userinput>yum upgrade</userinput></screen>

      <note>
        <para>Some services may terminate with an error during the package
        upgrade process. If this may cause a problem with your environment,
        consider stopping all services before upgrading them to Havana.</para>
      </note>

      <para>Install the OpenStack SELinux package on the controller
      node:</para>

      <screen><prompt>#</prompt> <userinput>yum install openstack-selinux</userinput></screen>

      <note>
        <para>The package manager will append <code>.rpmnew</code> to the end
        of newer versions of existing configuration files. You should consider
        adopting conventions associated with the newer configuration files and
        merging them with your existing configuration files after completing
        the upgrade process.</para>
      </note>
    </section>

    <section xml:id="upgrade_database_restart-rhel">
      <title>Stop Services, Update Database Schemas, and Restart Services on
      the Controller Node</title>

      <para>Stop each service, run the database synchronization command if
      necessary to update the associated database schema, and restart each
      service to apply the new configuration. Some services require additional
      commands:</para>

      <variablelist>
        <varlistentry>
          <term>OpenStack Identity</term>

          <listitem>
            <screen><prompt>#</prompt> <userinput>service openstack-keystone stop</userinput>
<prompt>#</prompt> <userinput>keystone-manage token_flush</userinput>
<prompt>#</prompt> <userinput>keystone-manage db_sync</userinput>
<prompt>#</prompt> <userinput>service openstack-keystone start</userinput></screen>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>OpenStack Image Service</term>

          <listitem>
            <screen><prompt>#</prompt> <userinput>service openstack-glance-api stop</userinput>
<prompt>#</prompt> <userinput>service openstack-glance-registry stop</userinput>
<prompt>#</prompt> <userinput>glance-manage db_sync</userinput>
<prompt>#</prompt> <userinput>service openstack-glance-api start</userinput>
<prompt>#</prompt> <userinput>service openstack-glance-registry start</userinput></screen>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>OpenStack Compute</term>

          <listitem>
            <screen><prompt>#</prompt> <userinput>service openstack-nova-api stop</userinput>
<prompt>#</prompt> <userinput>service openstack-nova-scheduler stop</userinput>
<prompt>#</prompt> <userinput>service openstack-nova-conductor stop</userinput>
<prompt>#</prompt> <userinput>service openstack-nova-cert stop</userinput>
<prompt>#</prompt> <userinput>service openstack-nova-consoleauth stop</userinput>
<prompt>#</prompt> <userinput>service openstack-nova-novncproxy stop</userinput>
<prompt>#</prompt> <userinput>nova-manage db sync</userinput>
<prompt>#</prompt> <userinput>service openstack-nova-api start</userinput>
<prompt>#</prompt> <userinput>service openstack-nova-scheduler start</userinput>
<prompt>#</prompt> <userinput>service openstack-nova-conductor start</userinput>
<prompt>#</prompt> <userinput>service openstack-nova-cert start</userinput>
<prompt>#</prompt> <userinput>service openstack-nova-consoleauth start</userinput>
<prompt>#</prompt> <userinput>service openstack-nova-novncproxy start</userinput></screen>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>OpenStack Block Storage</term>

          <listitem>
            <screen><prompt>#</prompt> <userinput>service openstack-cinder-api stop</userinput>
<prompt>#</prompt> <userinput>service openstack-cinder-scheduler stop</userinput>
<prompt>#</prompt> <userinput>cinder-manage db sync</userinput>
<prompt>#</prompt> <userinput>service openstack-cinder-api start</userinput>
<prompt>#</prompt> <userinput>service openstack-cinder-scheduler start</userinput></screen>
          </listitem>
        </varlistentry>
      </variablelist>

      <para>The controller node update is complete. Now you can upgrade the
      compute nodes.</para>
    </section>

    <section xml:id="upgrade_packages_compute-rhel">
      <title>Upgrade Packages and Restart Services on the Compute
      Nodes</title>

      <para>Upgrade packages on the compute nodes to Havana:</para>

      <screen><prompt>#</prompt> <userinput>yum upgrade</userinput></screen>

      <note>
        <para>Make sure you have removed the repository for Grizzly packages
        and added the repository for Havana packages.</para>
      </note>

      <para>Install the OpenStack SELinux package on the compute nodes:</para>

      <screen><prompt>#</prompt> <userinput>yum install openstack-selinux</userinput></screen>

      <para>Restart compute services:</para>

      <screen><prompt>#</prompt> <userinput>service openstack-nova-compute restart</userinput>
<prompt>#</prompt> <userinput>service openstack-nova-network restart</userinput>
<prompt>#</prompt> <userinput>service openstack-nova-metadata-api restart</userinput></screen>
    </section>

    <section xml:id="upgrade_packages_storage-rhel">
      <title>Upgrade Packages and Restart Services on the Block Storage
      Nodes</title>

      <para>Upgrade packages on the storage nodes to Havana:</para>

      <screen><prompt>#</prompt> <userinput>yum upgrade</userinput></screen>

      <note>
        <para>Make sure you have removed the repository for Grizzly packages
        and added the repository for Havana packages.<indexterm
        class="endofrange" startref="UPredhat" /></para>
      </note>

      <para>Install the OpenStack SELinux package on the storage nodes:</para>

      <screen><prompt>#</prompt> <userinput>yum install openstack-selinux</userinput></screen>

      <para>Restart Block Storage services:</para>

      <screen><prompt>#</prompt> <userinput>service openstack-cinder-volume restart</userinput></screen>
    </section>
  </section>
    <section xml:id="upgrades_havana-icehouse-ubuntu">
      <title>How to Perform an Upgrade from Havana to Icehouse—Ubuntu</title>
      <?dbhtml stop-chunking?>
      <para>For this section, we assume that you are starting with the
        architecture provided in the OpenStack <link
          xlink:href="http://docs.openstack.org/havana/install-guide/install/apt/content/"
        >Installation Guide</link> and upgrading to the same
        architecture for Icehouse. All nodes should run Ubuntu 12.04 LTS
        with Linux kernel 3.11 and the latest Havana packages installed and
        operational. This section primarily addresses upgrading core
        OpenStack services such as Identity (keystone), Image Service
        (glance), Compute (nova), Networking (neutron), Block Storage
        (cinder), and the dashboard. The Networking upgrade includes
        conversion from the Open vSwitch (OVS) plug-in to the Modular
        Layer 2 (M2) plug-in. This section does not cover the upgrade
        process from Ubuntu 12.04 LTS to Ubuntu 14.04 LTS.</para>
    <section xml:id="upgrade_icehouse-ubuntu-impact">
      <title>Impact on Users</title>
      <para>The upgrade process will interrupt management of your
        environment, including the dashboard. If you properly prepare
        for this upgrade, tenant instances will continue to operate
        normally.</para>
    </section>
    <section xml:id="upgrade_icehouse-ubuntu-considerations">
      <title>Upgrade Considerations</title>
      <para>Always review the <link
          xlink:href="https://wiki.openstack.org/wiki/ReleaseNotes/Icehouse"
        >release notes</link> before performing an upgrade to learn about
        newly available features that you may want to enable and deprecated
        features that you should disable.</para>
    </section>
    <section xml:id="upgrade_icehouse-ubuntu-backup">
      <title>Perform a Backup</title>
      <para>Save the configuration files on all nodes, as shown here:</para>
      <screen><prompt>#</prompt> <userinput>for i in keystone glance nova cinder neutron openstack-dashboard</userinput>
<prompt>&gt;</prompt> <userinput>do mkdir $i-havana</userinput>
<prompt>&gt;</prompt> <userinput>done</userinput>
<prompt>#</prompt> <userinput>for i in keystone glance nova cinder neutron openstack-dashboard</userinput>
<prompt>&gt;</prompt> <userinput>do cp -r /etc/$i/* $i-havana/</userinput>
<prompt>&gt;</prompt> <userinput>done</userinput></screen>
      <note>
        <para>You can modify this example script on each node to handle
          different services.</para>
      </note>
      <para>Back up all databases on the controller:</para>
      <screen><prompt>#</prompt> <userinput>mysqldump -u root -p --opt --add-drop-database --all-databases &gt; havana-db-backup.sql</userinput></screen>
      <note>
        <para>Although not necessary, you should consider updating your
          MySQL server configuration as described in the
          <link xlink:href="http://docs.openstack.org/icehouse/install-guide/install/apt/content/basics-database-controller.html"
            >MySQL controller setup</link> section of the
          <link xlink:href="http://docs.openstack.org/icehouse/install-guide/install/apt/content/"
            >Installation Guide</link>.</para>
      </note>
    </section>
    <section xml:id="upgrade_icehouse-ubuntu-manage-repos">
      <title>Manage Repositories</title>
      <para>On all nodes, remove the repository for Havana packages and
        add the repository for Icehouse packages:</para>
      <screen><prompt>#</prompt> <userinput>apt-add-repository -r cloud-archive:havana</userinput>
<prompt>#</prompt> <userinput>apt-add-repository cloud-archive:icehouse</userinput></screen>
      <warning>
        <para>Disable any <link xlink:href="https://help.ubuntu.com/12.04/serverguide/automatic-updates.html">automatic package updates</link>.</para>
      </warning>
    </section>
    <section xml:id="upgrade_icehouse-ubuntu-notes">
      <title>Upgrade Notes</title>
      <itemizedlist>
        <listitem>
          <para>Disable Compute file injection:</para>
          <para>Icehouse disables file injection by default per the
            <link xlink:href="https://wiki.openstack.org/wiki/ReleaseNotes/Icehouse"
            >release notes</link>.</para>
          <para>If you plan to deploy Icehouse in stages, you must
            disable file injection on all compute nodes that
            will remain on Havana.</para>
          <para>Edit the
            <filename>/etc/nova/nova-compute.conf</filename>
            file:</para>
          <programlisting language="ini">[libvirt]
...
libvirt_inject_partition = -2</programlisting>
        </listitem>
        <listitem>
          <para>Convert from OVS to ML2 plug-in:</para>
          <para>You must convert the configuration for your
            environment contained in the
            <filename>/etc/neutron/neutron.conf</filename> and
            <filename>/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini</filename>
            files from OVS to ML2. For example, the
            <link xlink:href="http://docs.openstack.org/icehouse/install-guide/install/apt/content/"
              >Installation Guide</link> covers
            <link xlink:href="http://docs.openstack.org/icehouse/install-guide/install/apt/content/section_neutron-networking-ml2.html"
              >ML2 plug-in configuration</link> using GRE tunnels.</para>
          <para>We recommend keeping the OVS plug-in packages and
            configuration files until you verify the upgrade.</para>
        </listitem>
      </itemizedlist>
    </section>
    <section xml:id="upgrade_icehouse-ubuntu-controller-node">
      <title>Upgrade the Controller Node</title>
      <para>Upgrade packages on the controller node to Havana, as shown
        below:</para>
      <screen><prompt>#</prompt> <userinput>apt-get update</userinput>
<prompt>#</prompt> <userinput>apt-get dist-upgrade</userinput></screen>
      <note>
        <para>Depending on your specific configuration, performing a
          <code>dist-upgrade</code> may restart services supplemental
          to your OpenStack environment. For example, if you use
          Open-iSCSI for Block Storage volumes and the upgrade includes
          a new <code>open-scsi</code> package, the package manager
          will restart Open-iSCSI services, which may cause the
          volumes for your users to be disconnected.</para>
      </note>
      <para>The package manager will ask you about updating various
        configuration files. We recommend denying these changes. The
        package manager will append <code>.dpkg-dist</code> to the
        end of newer versions of existing configuration files. You
        should consider adopting conventions associated with the
        newer configuration files and merging them with your existing
        configuration files after completing the upgrade process.
        One way to quickly find what configuration files are yet to
        be merged is using the following command:</para>
      <screen><prompt>#</prompt> <userinput>find /etc -name *.dpkg-dist</userinput></screen>
    </section>
    <section xml:id="upgrade_icehouse-ubuntu-services">
      <title>Upgrade Each Service</title>
      <para>The upgrade procedure for each service typically requires
        that you stop the service, run the database synchronization
        command to update the associated database, and start the
        service to apply the new configuration. You will need
        administrator privileges for these procedures. Some services
        require additional steps.</para>
      <itemizedlist>
        <listitem>
          <para>OpenStack Identity:</para>
          <para>Stop services, upgrade the database, and start
            services.</para>
          <screen><prompt>#</prompt> <userinput>service keystone stop</userinput>
<prompt>#</prompt> <userinput>keystone-manage token_flush</userinput>
<prompt>#</prompt> <userinput>keystone-manage db_sync</userinput>
<prompt>#</prompt> <userinput>service keystone start</userinput></screen>
        </listitem>
        <listitem>
          <para>OpenStack Image Service:</para>
          <para>Before upgrading the Image Service database, you
            must convert the character set for each table to UTF-8.</para>
          <para>Use the MySQL client to execute the following
            commands:</para>
          <screen><prompt>#</prompt> <userinput>mysql -u root -p</userinput>
<prompt>mysql></prompt> <userinput>SET foreign_key_checks = 0;</userinput>
<prompt>mysql></prompt> <userinput>ALTER TABLE glance.image_locations CONVERT TO CHARACTER SET 'utf8';</userinput>
<prompt>mysql></prompt> <userinput>ALTER TABLE glance.image_members CONVERT TO CHARACTER SET 'utf8';</userinput>
<prompt>mysql></prompt> <userinput>ALTER TABLE glance.image_properties CONVERT TO CHARACTER SET 'utf8';</userinput>
<prompt>mysql></prompt> <userinput>ALTER TABLE glance.image_tags CONVERT TO CHARACTER SET 'utf8';</userinput>
<prompt>mysql></prompt> <userinput>ALTER TABLE glance.images CONVERT TO CHARACTER SET 'utf8';</userinput>
<prompt>mysql></prompt> <userinput>ALTER TABLE glance.migrate_version CONVERT TO CHARACTER SET 'utf8';</userinput>
<prompt>mysql></prompt> <userinput>SET foreign_key_checks = 1;</userinput>
<prompt>mysql></prompt> <userinput>exit</userinput></screen>
          <note>
            <para>Your environment might contain different or
              additional tables that you must also convert to
              UTF-8 using similar commands.</para>
          </note>
          <para>Update the configuration for compatibility with
            Icehouse.</para>
          <para>Edit the
            <filename>/etc/glance/glance-api.conf</filename> and
            <filename>/etc/glance/glance-registry.conf</filename>
            files:</para>
          <para>Add the <literal>[database]</literal> section.</para>
          <para>Rename the <literal>sql_connection</literal> key to
            <literal>connection</literal> and move it to the
            <literal>[database]</literal> section.</para>
          <para>Edit the
            <filename>/etc/glance/glance-api.conf</filename> file:
          </para>
          <para>Replace <replaceable>RABBIT_PASS</replaceable> with
            with the password you chose for the
            <literal>guest</literal> account in
            <application>RabbitMQ</application>.</para>
          <programlisting language="ini">[DEFAULT]
...
rpc_backend = rabbit
rabbit_host = <replaceable>controller</replaceable>
rabbit_password = <replaceable>RABBIT_PASS</replaceable></programlisting>
          <para>Stop services, upgrade the database, and start
            services:</para>
          <screen><prompt>#</prompt> <userinput>service glance-api stop</userinput>
<prompt>#</prompt> <userinput>service glance-registry stop</userinput>
<prompt>#</prompt> <userinput>glance-manage db_sync</userinput>
<prompt>#</prompt> <userinput>service glance-api start</userinput>
<prompt>#</prompt> <userinput>service glance-registry start</userinput></screen>
        </listitem>
        <listitem>
          <para>OpenStack Compute:</para>
          <para>Update the configuration for compatibility with
            Icehouse.</para>
          <para>Edit the
            <filename>/etc/nova/nova.conf</filename> file:</para>
            <para>Change the <literal>rpc_backend</literal> key from
            <literal>nova.rpc.impl_kombu</literal> to
            <literal>rabbit</literal>.</para>
          <para>Edit the
            <filename>/etc/nova/api-paste.ini</filename> file:</para>
          <para>Comment out or remove any keys in the
            <literal>[filter:authtoken]</literal> section below
            <literal>paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory</literal>.</para>
          <para>Stop services, upgrade the database, and start
            services.</para>
          <screen><prompt>#</prompt> <userinput>service nova-api stop</userinput>
<prompt>#</prompt> <userinput>service nova-scheduler stop</userinput>
<prompt>#</prompt> <userinput>service nova-conductor stop</userinput>
<prompt>#</prompt> <userinput>service nova-cert stop</userinput>
<prompt>#</prompt> <userinput>service nova-consoleauth stop</userinput>
<prompt>#</prompt> <userinput>service nova-novncproxy stop</userinput>
<prompt>#</prompt> <userinput>nova-manage db sync</userinput>
<prompt>#</prompt> <userinput>service nova-api start</userinput>
<prompt>#</prompt> <userinput>service nova-scheduler start</userinput>
<prompt>#</prompt> <userinput>service nova-conductor start</userinput>
<prompt>#</prompt> <userinput>service nova-cert start</userinput>
<prompt>#</prompt> <userinput>service nova-consoleauth start</userinput>
<prompt>#</prompt> <userinput>service nova-novncproxy start</userinput></screen>
        </listitem>
        <listitem>
          <para>OpenStack Networking:</para>
          <para>Populate the
            <filename>/etc/neutron/plugins/ml2/ml2_conf.ini</filename>
            file with the equivalent configuration for your
            environment. Do not edit the
            <filename>/etc/neutron/neutron.conf</filename> file
            until after the conversion steps.</para>
          <para>Stop services, upgrade the database, and perform the
            conversion from OVS to ML2.</para>
          <para>Replace <replaceable>NEUTRON_DBPASS</replaceable>
            with the password you chose for the database.</para>
          <warning>
            <para>We highly recommend that you perform a database
              backup prior to executing the following commands as
              the conversion script cannot roll back.</para>
          </warning>
          <screen><prompt>#</prompt> <userinput>service neutron-server stop</userinput>
<prompt>#</prompt> <userinput>neutron-db-manage --config-file /etc/neutron/neutron.conf \
  --config-file /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini stamp havana</userinput>
<prompt>#</prompt> <userinput>neutron-db-manage --config-file /etc/neutron/neutron.conf \
  --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade icehouse</userinput>
<prompt>#</prompt> <userinput>python -m neutron.db.migration.migrate_to_ml2 openvswitch \
  mysql://neutron:NEUTRON_DBPASS@controller/neutron</userinput></screen>
          <para>Edit the
            <filename>/etc/neutron/neutron.conf</filename> file
            to use the ML2 plug-in:</para>
          <para>Replace <replaceable>SERVICE_TENANT_ID</replaceable>
            with the service tenant identifier (id) in the Identity
            service and <replaceable>NOVA_PASS</replaceable> with
            the password you chose for the <literal>nova</literal>
            user in the Identity service.</para>
          <programlisting language="ini">[DEFAULT]
...
core_plugin = ml2
service_plugins = router
...
notify_nova_on_port_status_changes = True
notify_nova_on_port_data_changes = True
nova_url = http://<replaceable>controller</replaceable>:8774/v2
nova_admin_username = nova
nova_admin_tenant_id = <replaceable>SERVICE_TENANT_ID</replaceable>
nova_admin_password = <replaceable>NOVA_PASS</replaceable>
nova_admin_auth_url = http://<replaceable>controller</replaceable>:35357/v2.0
</programlisting>
          <para>Start Networking services.</para>
          <screen><prompt>#</prompt> <userinput>service neutron-server start</userinput></screen>
        </listitem>
        <listitem>
          <para>OpenStack Block Storage:</para>
          <para>Stop services, upgrade the database, and start
            services.</para>
          <screen><prompt>#</prompt> <userinput>service cinder-api stop</userinput>
<prompt>#</prompt> <userinput>service cinder-volume stop</userinput>
<prompt>#</prompt> <userinput>service cinder-scheduler stop</userinput>
<prompt>#</prompt> <userinput>cinder-manage db sync</userinput>
<prompt>#</prompt> <userinput>service cinder-api start</userinput>
<prompt>#</prompt> <userinput>service cinder-volume start</userinput>
<prompt>#</prompt> <userinput>service cinder-scheduler start</userinput></screen>
        </listitem>
        <listitem>
          <para>Dashboard:</para>
          <para>Update the configuration for compatibility with
            Icehouse.</para>
          <para>Edit the
            <filename>/etc/openstack-dashboard/local_settings.py</filename> file:</para>
          <para>Change the
            <literal>OPENSTACK_KEYSTONE_DEFAULT_ROLE</literal> key
            from <literal>"Member"</literal> to
            <literal>"_member_"</literal>.</para>
          <para>Restart Dashboard services.</para>
          <screen><prompt>#</prompt> <userinput>service apache2 restart</userinput></screen>
        </listitem>
      </itemizedlist>
      <para>The controller node update is complete. Now you can upgrade
        the remaining nodes.</para>
    </section>
    <section xml:id="upgrade_icehouse-ubuntu-network-node">
      <title>Upgrade the Network Node</title>
      <para>Upgrade packages on the network node to Icehouse:</para>
      <note>
        <para>Make sure you have removed the repository for Havana
          packages and added the repository for Icehouse packages.</para>
      </note>
      <screen><prompt>#</prompt> <userinput>apt-get update</userinput>
<prompt>#</prompt> <userinput>apt-get dist-upgrade</userinput></screen>
      <para>Edit the
        <filename>/etc/neutron/neutron.conf</filename> file
        to use the ML2 plug-in:</para>
      <programlisting language="ini">[DEFAULT]
core_plugin = ml2
service_plugins = router</programlisting>
      <para>Populate the
        <filename>/etc/neutron/plugins/ml2/ml2_conf.ini</filename>
        file with the equivalent configuration for your
        environment.</para>
      <para>Clean the active OVS configuration:</para>
      <screen><prompt>#</prompt> <userinput>service neutron-ovs-cleanup restart</userinput></screen>
      <para>Restart Networking services:</para>
      <screen><prompt>#</prompt> <userinput>service neutron-dhcp-agent restart</userinput>
<prompt>#</prompt> <userinput>service neutron-l3-agent restart</userinput>
<prompt>#</prompt> <userinput>service neutron-metadata-agent restart</userinput>
<prompt>#</prompt> <userinput>service neutron-plugin-openvswitch-agent restart</userinput></screen>
    </section>
    <section xml:id="upgrade_icehouse-ubuntu-compute-nodes">
      <title>Upgrade the Compute Nodes</title>
      <para>Upgrade packages on the compute nodes to Icehouse:</para>
      <note>
        <para>Make sure you have removed the repository for Havana
          packages and added the repository for Icehouse packages.</para>
      </note>
      <screen><prompt>#</prompt> <userinput>apt-get update</userinput>
<prompt>#</prompt> <userinput>apt-get dist-upgrade</userinput></screen>
      <para>Edit the
        <filename>/etc/neutron/neutron.conf</filename> file
        to use the ML2 plug-in:</para>
      <programlisting language="ini">[DEFAULT]
core_plugin = ml2
service_plugins = router</programlisting>
      <para>Populate the
        <filename>/etc/neutron/plugins/ml2/ml2_conf.ini</filename>
        file with the equivalent configuration for your
        environment.</para>
      <para>Clean the active OVS configuration:</para>
      <screen><prompt>#</prompt> <userinput>service neutron-ovs-cleanup restart</userinput></screen>
      <para>Restart Networking services:</para>
      <screen><prompt>#</prompt> <userinput>service neutron-plugin-openvswitch-agent restart</userinput></screen>
      <para>Restart Compute services:</para>
      <screen><prompt>#</prompt> <userinput>service nova-compute restart</userinput></screen>
    </section>
    <section xml:id="upgrade_icehouse-ubuntu-storage-nodes">
      <title>Upgrade the Storage Nodes</title>
      <para>Upgrade packages on the storage nodes to Icehouse:</para>
      <screen><prompt>#</prompt> <userinput>apt-get update</userinput>
<prompt>#</prompt> <userinput>apt-get dist-upgrade</userinput></screen>
      <note>
        <para>Make sure you have removed the repository for Havana
          packages and added the repository for Icehouse packages.</para>
      </note>
      <para>Restart Block Storage services.</para>
      <screen><prompt>#</prompt> <userinput>service cinder-volume restart</userinput></screen>
    </section>
  </section>

  <section xml:id="ops_upgrades-final-steps">
    <title>Cleaning Up and Final Configuration File Updates</title>

    <para>On all distributions, you need to perform some final tasks to
    complete the upgrade process.<indexterm class="singular">
        <primary>upgrading</primary>

        <secondary>final steps</secondary>
      </indexterm></para>

    <para>Decrease DHCP timeouts by modifying
    <filename>/etc/nova/nova.conf</filename> on the compute nodes back to the
    original value for your environment.</para>

    <para>Update all of the <filename>.ini</filename> files to match passwords
    and pipelines as required for Havana in your environment.</para>

    <para>After a migration, your users will see different results from
    <systemitem class="service">nova image-list</systemitem> and <systemitem
    class="service">glance image-list</systemitem> unless you match up
    policies for access to private images. To do so, edit
    <filename>/etc/glance/policy.json</filename> and
    <filename>/etc/nova/policy.json</filename> to contain
    <code>"context_is_admin": "role:admin",</code> which limits access to
    private images for projects.</para>

    <para>Thoroughly test the environment, and then let your users know that
    their cloud is running normally again.</para>
  </section>

  <section xml:id="ops_upgrades-roll-back">
    <title>Rolling Back a Failed Upgrade</title>

    <para>While we do not wish this fate upon anyone, upgrades involve complex
    operations and can fail. This section provides guidance for rolling back
    to a previous release of OpenStack. Although only tested on Ubuntu, other
    distributions follow a similar <phrase
    role="keep-together">procedure</phrase>.<indexterm class="singular">
        <primary>rollbacks</primary>

        <secondary>process for</secondary>
      </indexterm><indexterm class="singular">
        <primary>upgrading</primary>

        <secondary>rolling back failures</secondary>
      </indexterm></para>

    <para>In this section, we consider only the most immediate case: you have
    taken down production management services in preparation for an upgrade,
    completed part of the upgrade process, discovered one or more problems not
    encountered during testing, and need to roll back your environment to the
    original "known good" state. We specifically assume that you did not make
    any state changes after attempting the upgrade process: no new instances,
    networks, storage volumes, etc.</para>

    <para>Within this scope, you need to accomplish three main steps to
    successfully roll back your environment:</para>

    <itemizedlist>
      <listitem>
        <para>Roll back configuration files</para>
      </listitem>

      <listitem>
        <para>Roll back databases</para>
      </listitem>

      <listitem>
        <para>Roll back packages</para>
      </listitem>
    </itemizedlist>

    <para>The upgrade instructions provided in earlier sections ensure that
    you have proper backups of your databases and configuration files. You
    should read through this section carefully and verify that you have the
    requisite backups to restore. Rolling back upgrades is a tricky process
    because distributions tend to put much more effort into testing upgrades
    than downgrades. Broken downgrades often take significantly more effort to
    troubleshoot and, hopefully, resolve than broken upgrades. Only you can
    weigh the risks of trying to push a failed upgrade forward versus rolling
    it back. Generally, we consider rolling back the very last option.</para>

    <para>The following steps described for Ubuntu have worked on at least one
    production environment, but they may not work for all environments.</para>

    <procedure>
      <title>Perform the rollback from Havana to Grizzly</title>

      <step>
        <para>Stop all OpenStack services.</para>
      </step>

      <step>
        <para>Copy contents of configuration backup directories
        <filename>/etc/&lt;service&gt;.grizzly</filename> that you created
        during the upgrade process back to
        <filename>/etc/&lt;service&gt;</filename>:</para>
      </step>

      <step>
        <para>Restore databases from the backup file
        <filename>grizzly-db-backup.sql</filename> that you created with
        <literal>mysqldump</literal> during the upgrade process:</para>

        <screen><prompt>#</prompt> <userinput>mysql -u root -p &lt; grizzly-db-backup.sql</userinput></screen>

        <para>If you created this backup using the
        <literal>--add-drop-database</literal> flag as instructed, you can
        proceed to the next step. If you omitted this flag, MySQL will revert
        all of the tables that existed in Grizzly, but not drop any tables
        created during the database migration for Havana. In this case, you
        need to manually determine which tables should not exist and drop them
        to prevent issues with your next upgrade <phrase
        role="keep-together">attempt</phrase>.</para>
      </step>

      <step>
        <para>Downgrade OpenStack packages.</para>

        <warning>
          <para>We consider downgrading packages by far the most complicated
          step; it is highly dependent on the distribution as well as overall
          administration of the system.</para>
        </warning>

        <substeps>
          <step>
            <para>Determine the OpenStack packages installed on your system.
            This is done using <literal>dpkg --get-selections</literal>,
            filtering for OpenStack packages, filtering again to omit packages
            explicitly marked in the <code>deinstall</code> state, and saving
            the final output to a file. For example, the following command
            covers a controller node with keystone, glance, nova, neutron, and
            cinder:</para>

            <screen><prompt>#</prompt> <userinput>dpkg --get-selections | grep -e keystone -e glance -e nova -e neutron \
-e cinder | grep -v deinstall | tee openstack-selections</userinput>
<computeroutput>cinder-api                                      install
cinder-common                                   install
cinder-scheduler                                install
cinder-volume                                   install
glance                                          install
glance-api                                      install
glance-common                                   install
glance-registry                                 install
neutron-common                                  install
neutron-dhcp-agent                              install
neutron-l3-agent                                install
neutron-lbaas-agent                             install
neutron-metadata-agent                          install
neutron-plugin-openvswitch                      install
neutron-plugin-openvswitch-agent                install
neutron-server                                  install
nova-api                                        install
nova-cert                                       install
nova-common                                     install
nova-conductor                                  install
nova-consoleauth                                install
nova-novncproxy                                 install
nova-objectstore                                install
nova-scheduler                                  install
python-cinder                                   install
python-cinderclient                             install
python-glance                                   install
python-glanceclient                             install
python-keystone                                 install
python-keystoneclient                           install
python-neutron                                  install
python-neutronclient                            install
python-nova                                     install
python-novaclient                               install
</computeroutput></screen>

            <note>
              <para>Depending on the type of server, the contents and order of
              your package list may vary from this example.</para>
            </note>
          </step>

          <step>
            <para>You can determine the package versions available for
            reversion by using <literal>apt-cache policy</literal>. If you
            removed the Grizzly repositories, you must first reinstall them
            and run <literal>apt-get update</literal>:</para>

            <!-- FIXME - there was a query about whether this command and the output is
aligned correctly. In the PDF the # is directly above the n of nova common, and
everything is indented below the m of them in the previous sentence -->

            <screen><prompt>#</prompt> <userinput>apt-cache policy nova-common</userinput>
<computeroutput>nova-common:
  Installed: 1:2013.2-0ubuntu1~cloud0
  Candidate: 1:2013.2-0ubuntu1~cloud0
  Version table:
 *** 1:2013.2-0ubuntu1~cloud0 0
        500 http://ubuntu-cloud.archive.canonical.com/ubuntu/
            precise-updates/havana/main amd64 Packages
        100 /var/lib/dpkg/status
     1:2013.1.4-0ubuntu1~cloud0 0
        500 http://ubuntu-cloud.archive.canonical.com/ubuntu/
            precise-updates/grizzly/main amd64 Packages
     2012.1.3+stable-20130423-e52e6912-0ubuntu1.2 0
        500 http://us.archive.ubuntu.com/ubuntu/
            precise-updates/main amd64 Packages
        500 http://security.ubuntu.com/ubuntu/
            precise-security/main amd64 Packages
     2012.1-0ubuntu2 0
        500 http://us.archive.ubuntu.com/ubuntu/
            precise/main amd64 Packages
</computeroutput></screen>

            <para>This tells us the currently installed version of the
            package, newest candidate version, and all versions along with the
            repository that contains each version. Look for the appropriate
            Grizzly version—<code>1:2013.1.4-0ubuntu1~cloud0</code> in this
            case. The process of manually picking through this list of
            packages is rather tedious and prone to errors. You should
            consider using the following script to help with this
            process:</para>

            <!-- FIXME - there was a query about whether this command and the output is
aligned correctly. -->

            <screen><prompt>#</prompt> <userinput>for i in `cut -f 1 openstack-selections | sed 's/neutron/quantum/;'`;
  do echo -n $i ;apt-cache policy $i | grep -B 1 grizzly |
  grep -v Packages | awk '{print "="$1}';done | tr '\n' ' ' |
  tee openstack-grizzly-versions</userinput>
<computeroutput>cinder-api=1:2013.1.4-0ubuntu1~cloud0
cinder-common=1:2013.1.4-0ubuntu1~cloud0
cinder-scheduler=1:2013.1.4-0ubuntu1~cloud0
cinder-volume=1:2013.1.4-0ubuntu1~cloud0
glance=1:2013.1.4-0ubuntu1~cloud0
glance-api=1:2013.1.4-0ubuntu1~cloud0
glance-common=1:2013.1.4-0ubuntu1~cloud0
glance-registry=1:2013.1.4-0ubuntu1~cloud0
quantum-common=1:2013.1.4-0ubuntu1~cloud0
quantum-dhcp-agent=1:2013.1.4-0ubuntu1~cloud0
quantum-l3-agent=1:2013.1.4-0ubuntu1~cloud0
quantum-lbaas-agent=1:2013.1.4-0ubuntu1~cloud0
quantum-metadata-agent=1:2013.1.4-0ubuntu1~cloud0
quantum-plugin-openvswitch=1:2013.1.4-0ubuntu1~cloud0
quantum-plugin-openvswitch-agent=1:2013.1.4-0ubuntu1~cloud0
quantum-server=1:2013.1.4-0ubuntu1~cloud0
nova-api=1:2013.1.4-0ubuntu1~cloud0
nova-cert=1:2013.1.4-0ubuntu1~cloud0
nova-common=1:2013.1.4-0ubuntu1~cloud0
nova-conductor=1:2013.1.4-0ubuntu1~cloud0
nova-consoleauth=1:2013.1.4-0ubuntu1~cloud0
nova-novncproxy=1:2013.1.4-0ubuntu1~cloud0
nova-objectstore=1:2013.1.4-0ubuntu1~cloud0
nova-scheduler=1:2013.1.4-0ubuntu1~cloud0
python-cinder=1:2013.1.4-0ubuntu1~cloud0
python-cinderclient=1:1.0.3-0ubuntu1~cloud0
python-glance=1:2013.1.4-0ubuntu1~cloud0
python-glanceclient=1:0.9.0-0ubuntu1.2~cloud0
python-quantum=1:2013.1.4-0ubuntu1~cloud0
python-quantumclient=1:2.2.0-0ubuntu1~cloud0
python-nova=1:2013.1.4-0ubuntu1~cloud0
python-novaclient=1:2.13.0-0ubuntu1~cloud0
</computeroutput></screen>

            <note>
              <para>If you decide to continue this step manually, don't forget
              to change <code>neutron</code> to <code>quantum</code> where
              applicable.</para>
            </note>
          </step>

          <step>
            <para>Use <literal>apt-get install</literal> to install specific
            versions of each package by specifying
            <code>&lt;package-name&gt;=&lt;version&gt;</code>. The script in
            the previous step conveniently created a list of
            <code>package=version</code> pairs for you:</para>

            <screen><prompt>#</prompt> <userinput>apt-get install `cat openstack-grizzly-versions`</userinput></screen>

            <para>This completes the rollback procedure. You should remove the
            Havana repository and run <literal>apt-get update</literal> to
            prevent accidental upgrades until you solve whatever issue caused
            you to roll back your environment.</para>
          </step>
        </substeps>
      </step>
    </procedure>
  </section>
</chapter>
